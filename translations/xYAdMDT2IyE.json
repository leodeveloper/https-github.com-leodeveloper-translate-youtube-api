{
    "source": "xYAdMDT2IyE",
    "youtubelink": "https://www.youtube.com/watch?v=xYAdMDT2IyE",
    "t_english": "- [Instructor] Algorithms increasingly control many areas of our everyday lives, from loan applications, to dating apps, to hospital waiting lists. As responsible consumers, and now creators of algorithms, we need to think critically about how the success of an algorithm gets measured. What assumptions and biases may be influencing the results, and how should that impact how we use them? Consider a recommendation algorithm. Your newsfeed doesn't ask you to rate your satisfaction with each result, so it doesn't truly know how you feel. Instead, many algorithms rate success based on more quantifiable indicators. For example, how often users click on a specific result. But this can lead to a false feedback loop where algorithms are more likely to recommend articles with sensational headlines or cute pictures of cats because they catch users' attention, even if when they do open the article, they're extremely dissatisfied with the result. Ultimately, this algorithm is measuring success based on engagement and not actual usefulness. That's not necessarily wrong, but it'll certainly influence the types of results that users see. So as you consume technology, keep a healthy skepticism and ask yourself, \"Why is the algorithm offering me this result?\" Okay, so humans are biased, but what if I just have AI design my algorithm for me? Well, AIs are trained on data that's created by humans, often content on the internet. Remember that computers see all data, whether it's textual, audio, or visual, as a sequence of numbers. They don't understand what's happening in a photo in the same way that a human does. Most of the time, AI-generated algorithms are just looking for patterns in the data whether there's a causal relationship or not. For example, an experimental hiring algorithm found that the AI favored male applicants, downgrading resumes that included terms like women's or referenced all women's colleges. The existing pool of engineers at the company was predominantly male, so when the AI trained on previous hiring data, it had found a pattern that it thought was meaningful, don't hire women. In practice, AI algorithms can actually amplify historical biases because available training data tends to lag behind the current cultural moment and heavily skews English, which means other cultures and languages are less represented. That's not to say that human-generated algorithms are always better than AI-generated ones, but AI algorithms tend to be less transparent, so it's more important that we hold organizations accountable for monitoring their bias. As programmers, how can we limit the bias in the algorithms we design? It's impossible to perfectly model the real world in a program. We'll always need to make some assumptions and some simplifications. We just wanna make sure we recognize the assumptions that we're making and we're comfortable with how that impacts our results. Let's evaluate this content moderation algorithm we wrote. What assumptions did we make? Well, here we're favoring older accounts, people who have been on the site for a while. Our algorithm assumes that those users are more trustworthy. Now, we looked at historical data and did find a correlation here, and we don't think account age correlates strongly with any protected class like race, gender, or religion. So I've decided I'm comfortable with this assumption. I'm willing to accept the slight unfairness toward well-intentioned new users. What about this word count check where assuming posts with a lot of words are less useful? Now, this might have an unfair impact based on the language of the post, because some languages tend to need more words to express the same idea. For example, French is often wordier than English. That's a bias I'm perhaps not willing to accept. Our site has a lot of users from all over the world, and I don't wanna favor one language over another. So I might go back to the drawing board with this one and either find a different criteria to use, or try to fairly adjust the word count limits based on the language. This evaluation process is ongoing. As this algorithm runs on our site, we wanna monitor trends in which posts are featured and which are flagged, and adjust the algorithm accordingly in response to new data.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
    "t_urdu": "- [انسٹرکٹر] الگورتھم\n ہماری روزمرہ کی زندگی کے بہت سے شعبوں کو تیزی سے کنٹرول کرتے ہیں، قرض کی درخواستوں سے لے کر ڈیٹنگ ایپس تک، ہسپتال کی انتظار کی فہرستوں تک۔ ذمہ دار صارفین کے طور پر، اور اب الگورتھم کے تخلیق کاروں، ہمیں اس بارے میں تنقیدی طور پر سوچنے کی ضرورت ہے کہ الگورتھم کی کامیابی کی پیمائش کیسے کی جاتی ہے۔\n کیا مفروضے اور تعصبات\nنتائج کو متاثر کر سکتے ہیں، اور اس کا\nاثر ہمارے ان کو استعمال کرنے کے طریقہ پر کیسے ہونا چاہیے؟ ایک تجویز کردہ الگورتھم پر غور کریں۔ آپ کی نیوز فیڈ آپ سے ہر ایک نتیجہ کے ساتھ آپ کے اطمینان کی درجہ بندی کرنے کے لیے نہیں کہتی،\n اس لیے یہ واقعی نہیں جانتی کہ آپ کیسا محسوس کرتے ہیں۔  اس کے بجائے، بہت سے الگورتھم زیادہ مقدار کے قابل اشاریوں کی بنیاد پر کامیابی کی درجہ بندی کرتے ہیں۔ مثال کے طور پر، صارفین کتنی بار\nکسی مخصوص نتیجہ پر کلک کرتے ہیں۔ لیکن یہ غلط فیڈ بیک لوپ کا باعث بن سکتا ہے جہاں الگورتھم\n سنسنی خیز\nسرخیوں یا بلیوں کی خوبصورت تصویروں والے مضامین کی تجویز کرنے کا زیادہ امکان رکھتے ہیں کیونکہ وہ صارفین کی توجہ حاصل کرتے ہیں، یہاں تک کہ جب وہ مضمون کھولتے ہیں، تو وہ\nنتیجہ سے انتہائی غیر مطمئن ہوتے ہیں۔ بالآخر، یہ الگورتھم\n مصروفیت کی بنیاد پر کامیابی کی پیمائش کر رہا ہے\nنہ کہ حقیقی افادیت کی بنیاد پر۔ یہ ضروری نہیں کہ غلط ہو،\nلیکن یہ یقینی طور پر ان نتائج کی اقسام کو متاثر کرے گا جو صارفین دیکھتے ہیں۔ لہذا جب آپ ٹیکنالوجی استعمال کرتے ہیں، صحت مند شکوک و شبہات کو برقرار رکھیں\nاور اپنے آپ سے پوچھیں، \"الگورتھم\nمجھے یہ نتیجہ کیوں پیش کر رہا ہے؟\" ٹھیک ہے، تو انسان متعصب ہوتے ہیں، لیکن کیا ہوگا اگر میرے پاس صرف AI\nمیرے لیے الگورتھم ڈیزائن کرے؟ ٹھیک ہے، AIs کو ایسے ڈیٹا پر تربیت دی جاتی ہے\nجو انسانوں کے ذریعہ تخلیق کیا جاتا ہے، اکثر انٹرنیٹ پر موجود مواد۔ یاد رکھیں کہ کمپیوٹر تمام ڈیٹا کو دیکھتے ہیں، چاہے وہ متنی ہو، آڈیو ہو یا بصری، نمبروں کی ترتیب کے طور پر۔ وہ نہیں سمجھتے کہ\nتصویر میں کیا ہو رہا ہے جیسا کہ انسان کرتا ہے۔ زیادہ تر وقت، AI سے تیار کردہ الگورتھم صرف اعداد و شمار میں پیٹرن تلاش کرتے ہیں کہ آیا اس میں کوئی وجہ\nتعلق ہے یا نہیں۔ مثال کے طور پر، ایک تجرباتی\nہائرنگ الگورتھم نے پایا کہ AI نے مرد درخواست دہندگان کی حمایت کی، ریزیوموں کو گھٹا دیا جس میں\nخواتین جیسی اصطلاحات شامل ہیں یا تمام خواتین کے کالجوں کا حوالہ دیا گیا ہے۔  کمپنی میں انجینئرز کا موجودہ پول\n بنیادی طور پر مرد تھا، لہذا جب AI نے\nپچھلے بھرتی کے اعداد و شمار پر تربیت حاصل کی، تو اسے ایک ایسا نمونہ ملا جو\nاس کے خیال میں معنی خیز تھا، خواتین کو ملازمت نہ دیں۔ عملی طور پر، AI الگورتھم درحقیقت تاریخی تعصبات کو بڑھا سکتے ہیں\nکیونکہ دستیاب تربیتی ڈیٹا \nموجودہ ثقافتی لمحے سے پیچھے رہ جاتا ہے اور انگریزی کو بہت زیادہ ترچھا دیتا ہے، جس کا مطلب ہے کہ دیگر ثقافتوں اور زبانوں کی نمائندگی کم ہے۔ اس کا مطلب یہ نہیں ہے کہ\nانسانی تخلیق کردہ الگورتھم ہمیشہ AI سے تیار کردہ سے بہتر ہوتے ہیں، لیکن AI الگورتھم\nکم شفاف ہوتے ہیں، اس لیے یہ زیادہ اہم ہے\nکہ ہم تنظیموں کو ان کے تعصب کی نگرانی کے لیے جوابدہ ٹھہرائیں۔ پروگرامرز کے طور پر، ہم اپنے ڈیزائن کردہ الگورتھم میں تعصب کو کیسے محدود کر سکتے ہیں؟ ایک پروگرام میں حقیقی دنیا کو مکمل طور پر ماڈل بنانا ناممکن ہے۔ ہمیں ہمیشہ کچھ مفروضے اور کچھ آسانیاں کرنے کی ضرورت ہوگی۔ ہم صرف اس بات کو یقینی بنانا چاہتے ہیں کہ ہم ان مفروضوں کو پہچانتے ہیں جو ہم کر رہے ہیں اور ہم اس بات سے مطمئن ہیں کہ\nیہ ہمارے نتائج پر کیسے اثر انداز ہوتا ہے۔ آئیے اس مواد کی\nاعتدال پسندی کے الگورتھم کا جائزہ لیں جو ہم نے لکھا ہے۔ ہم نے کیا مفروضے بنائے؟ ٹھیک ہے، یہاں ہم پرانے اکاؤنٹس کی حمایت کر رہے ہیں، وہ لوگ جو کچھ عرصے سے سائٹ پر موجود ہیں۔\n ہمارا الگورتھم فرض کرتا ہے کہ وہ\nصارفین زیادہ قابل اعتماد ہیں۔ اب، ہم نے تاریخی اعداد و شمار کو دیکھا اور یہاں ایک ارتباط پایا، اور ہمیں نہیں لگتا کہ اکاؤنٹ کی\nعمر کسی بھی محفوظ طبقے جیسے\nنسل، جنس، یا مذہب سے مضبوطی سے تعلق رکھتی ہے۔ لہذا میں نے فیصلہ کیا ہے کہ میں\nاس مفروضے سے راضی ہوں۔ میں\n نیک نیتی والے نئے صارفین کے ساتھ معمولی ناانصافی کو قبول کرنے کو تیار ہوں۔ اس لفظ کی گنتی کی جانچ کے بارے میں کیا خیال ہے جہاں\nبہت سارے الفاظ والی پوسٹس کم مفید ہیں؟ اب، پوسٹ کی زبان کی بنیاد پر اس کا غیر منصفانہ اثر ہو سکتا ہے، کیونکہ کچھ زبانوں کو\n ایک ہی خیال کے اظہار کے لیے زیادہ الفاظ کی ضرورت ہوتی ہے۔ مثال کے طور پر، فرانسیسی\nاکثر انگریزی سے زیادہ لفظی ہوتا ہے۔ یہ ایک تعصب ہے جسے میں شاید\nقبول کرنے کو تیار نہیں ہوں۔ ہماری سائٹ کے\nدنیا بھر سے بہت سارے صارفین ہیں، اور میں\nایک زبان کو دوسری زبان پر ترجیح نہیں دینا چاہتا۔ لہذا میں اس کے ساتھ ڈرائنگ بورڈ پر واپس جا سکتا ہوں\n اور یا تو\nاستعمال کرنے کے لیے ایک مختلف معیار تلاش کر سکتا ہوں، یا زبان کی بنیاد پر الفاظ کی گنتی کی حدوں کو مناسب طریقے سے ایڈجسٹ کرنے کی کوشش کر سکتا ہوں۔\n جانچ کا یہ عمل جاری ہے۔ چونکہ یہ الگورتھم ہماری سائٹ پر چلتا ہے، ہم ان رجحانات کی نگرانی کرنا چاہتے ہیں جن میں پوسٹس نمایاں ہیں\nاور جن پر پرچم لگایا گیا ہے، اور نئے ڈیٹا کے جواب میں الگورتھم کو اسی کے مطابق ایڈجسٹ کرنا چاہتے ہیں۔",
    "t_spanish": "- [Instructor] Los algoritmos\ncontrolan cada vez más muchas áreas de nuestra vida cotidiana, desde solicitudes de préstamos hasta aplicaciones de citas y listas de espera en hospitales. Como consumidores responsables, y ahora creadores de algoritmos, debemos pensar críticamente sobre cómo se mide el éxito de\nun algoritmo.  ¿ Qué suposiciones y sesgos\npueden estar influyendo en los resultados y cómo debería\nafectar eso a la forma en que los utilizamos? Considere un algoritmo de recomendación. Su suministro de noticias no le pide que califique su satisfacción\ncon cada resultado, por lo que no sabe realmente cómo se siente. En cambio, muchos algoritmos califican el éxito basándose en indicadores más cuantificables. Por ejemplo, la frecuencia con la que los usuarios\nhacen clic en un resultado específico. Pero esto puede llevar a un ciclo de retroalimentación falsa donde es\nmás probable que los algoritmos recomienden artículos con\ntitulares sensacionalistas o lindas imágenes de gatos porque captan la atención de los usuarios, incluso si cuando abren el artículo, están extremadamente\ninsatisfechos con el resultado. En última instancia, este algoritmo\nmide el éxito en función del compromiso y\nno de la utilidad real. Esto no es necesariamente incorrecto,\npero ciertamente influirá en los tipos de resultados que ven los usuarios. Así que mientras consumes tecnología, mantén un sano escepticismo\ny pregúntate: \"¿Por qué el algoritmo\nme ofrece este resultado?\". Bien, entonces los humanos son parciales, pero ¿qué pasa si simplemente hago que la IA\ndiseñe mi algoritmo por mí? Bueno, las IA se entrenan con datos\ncreados por humanos, a menudo contenidos en Internet. Recuerde que las computadoras ven todos los datos, ya sean textuales, de audio o visuales, como una secuencia de números.  No entienden\nlo que sucede en una foto de la misma manera que lo hace un humano. La mayoría de las veces, los algoritmos generados por IA solo buscan patrones en los datos, exista una\nrelación causal o no. Por ejemplo, un\nalgoritmo de contratación experimental descubrió que la IA favorecía a los solicitantes masculinos, degradando los currículums que\nincluían términos como mujeres o hacían referencia a todas las universidades de mujeres. El grupo existente de\ningenieros en la empresa era predominantemente masculino, por lo que cuando la IA se entrenó\ncon datos de contratación anteriores, encontró un patrón que\npensó que era significativo: no contratar mujeres. En la práctica, los algoritmos de IA pueden amplificar los sesgos históricos\nporque los datos de entrenamiento disponibles tienden a ir a la zaga del\nmomento cultural actual y sesgan mucho el inglés, lo que significa que otras culturas e idiomas están menos representados. Eso no quiere decir que los\nalgoritmos generados por humanos sean siempre mejores que los generados por IA, pero los algoritmos de IA tienden\na ser menos transparentes, por lo que es más importante\nque responsabilicemos a las organizaciones por monitorear sus prejuicios. Como programadores, ¿cómo podemos limitar el sesgo en los algoritmos que diseñamos? Es imposible modelar perfectamente el mundo real en un programa. Siempre necesitaremos hacer algunas suposiciones y algunas simplificaciones. Solo queremos asegurarnos de reconocer las suposiciones que hacemos y de sentirnos cómodos con el\nimpacto que eso tiene en nuestros resultados. Evaluemos este\nalgoritmo de moderación de contenido que escribimos.  ¿ Qué suposiciones hicimos? Bueno, aquí estamos favoreciendo a las cuentas más antiguas, personas que han estado en\nel sitio por un tiempo. Nuestro algoritmo asume que esos\nusuarios son más confiables. Ahora, analizamos datos históricos y encontramos una correlación aquí, y no creemos que la\nedad de la cuenta se correlacione fuertemente con ninguna clase protegida como\nraza, género o religión. Así que he decidido que me siento cómodo\ncon esta suposición. Estoy dispuesto a aceptar\nla ligera injusticia hacia los nuevos usuarios bien intencionados.  ¿ Qué pasa con esta verificación del recuento de palabras en la que asumir publicaciones con\nmuchas palabras es menos útil? Ahora bien, esto podría tener un impacto injusto según el idioma de la publicación, porque algunos idiomas\ntienden a necesitar más palabras para expresar la misma idea. Por ejemplo, el francés\nsuele tener más palabras que el inglés. Ése es un prejuicio que quizás\nno esté dispuesto a aceptar. Nuestro sitio tiene muchos usuarios\nde todo el mundo y no quiero favorecer\nun idioma sobre otro. Así que podría volver a la\nmesa de dibujo con este y encontrar un\ncriterio diferente para usar o intentar ajustar de manera justa\nlos límites del recuento de palabras según el idioma. Este proceso de evaluación está en curso. A medida que este algoritmo se ejecuta en nuestro sitio, queremos monitorear las tendencias en qué publicaciones se destacan\ny cuáles se marcan, y ajustar el algoritmo en consecuencia en respuesta a nuevos datos.",
    "t_arabic": "- [المدرس]\nتتحكم الخوارزميات بشكل متزايد في العديد من مجالات حياتنا اليومية، بدءًا من طلبات القروض وتطبيقات المواعدة وقوائم الانتظار في المستشفيات. باعتبارنا مستهلكين مسؤولين، والآن منشئي الخوارزميات، نحتاج إلى التفكير بشكل نقدي حول كيفية\nقياس نجاح الخوارزمية. ما هي الافتراضات والتحيزات التي\nقد تؤثر على النتائج، وكيف ينبغي أن\nيؤثر ذلك على كيفية استخدامنا لها؟ النظر في خوارزمية التوصية. لا يطلب منك ملف الأخبار الخاص بك تقييم مدى رضاك\nعن كل نتيجة، لذلك فهو لا يعرف حقًا ما تشعر به. وبدلاً من ذلك، تقوم العديد من الخوارزميات بتقييم النجاح بناءً على مؤشرات أكثر قابلية للقياس الكمي. على سبيل المثال، عدد المرات التي\nينقر فيها المستخدمون على نتيجة معينة. ولكن هذا يمكن أن يؤدي إلى حلقة ردود فعل زائفة حيث من\nالمرجح أن توصي الخوارزميات بمقالات ذات\nعناوين مثيرة أو صور لطيفة للقطط لأنها تجذب انتباه المستخدمين، حتى لو كانوا\nغير راضين تمامًا عن النتيجة عندما يفتحون المقالة. في نهاية المطاف، تقوم هذه الخوارزمية\nبقياس النجاح بناءً على المشاركة\nوليس على الفائدة الفعلية. وهذا ليس خطأً بالضرورة،\nلكنه سيؤثر بالتأكيد على أنواع النتائج التي يراها المستخدمون. لذا، بينما تستهلك التكنولوجيا، حافظ على شكوكك الصحية\nواسأل نفسك، \"لماذا\nتقدم لي الخوارزمية هذه النتيجة؟\" حسنًا، البشر متحيزون، لكن ماذا لو كان لدي الذكاء الاصطناعي\nلتصميم الخوارزمية الخاصة بي؟ حسنًا، يتم تدريب الذكاء الاصطناعي على البيانات\nالتي أنشأها البشر، والتي غالبًا ما تكون موجودة على الإنترنت. تذكر أن أجهزة الكمبيوتر ترى كافة البيانات، سواء كانت نصية أو صوتية أو مرئية، كسلسلة من الأرقام. إنهم لا يفهمون\nما يحدث في الصورة بنفس الطريقة التي يفهمها الإنسان.  في معظم الأحيان، تبحث الخوارزميات التي ينشئها الذكاء الاصطناعي فقط عن أنماط في البيانات سواء كانت هناك\nعلاقة سببية أم لا. على سبيل المثال، وجدت\nخوارزمية توظيف تجريبية أن الذكاء الاصطناعي يفضل المتقدمين الذكور، مما يقلل من مستوى السير الذاتية التي\nتتضمن مصطلحات مثل \"النساء\" أو تشير إلى جميع كليات البنات.  كان طاقم\nالمهندسين الحالي في الشركة في الغالب من الذكور، لذلك عندما تدرب الذكاء الاصطناعي\nعلى بيانات التوظيف السابقة، وجد نمطًا\nاعتقد أنه ذو معنى، لا تقم بتوظيف النساء. من الناحية العملية، يمكن لخوارزميات الذكاء الاصطناعي في الواقع تضخيم التحيزات التاريخية\nلأن بيانات التدريب المتاحة تميل إلى التخلف عن\nاللحظة الثقافية الحالية وتحرف اللغة الإنجليزية بشكل كبير، مما يعني أن الثقافات واللغات الأخرى أقل تمثيلاً. هذا لا يعني أن\nالخوارزميات التي ينشئها الإنسان هي دائمًا أفضل من تلك التي ينشئها الذكاء الاصطناعي، لكن خوارزميات الذكاء الاصطناعي تميل إلى أن\nتكون أقل شفافية، لذلك من المهم\nأن نحمل المؤسسات المسؤولية عن مراقبة تحيزها. كمبرمجين، كيف يمكننا الحد من التحيز في الخوارزميات التي نصممها؟  من المستحيل نمذجة العالم الحقيقي بشكل مثالي في البرنامج. سنحتاج دائمًا إلى وضع بعض الافتراضات وبعض التبسيطات. نريد فقط التأكد من أننا ندرك الافتراضات التي نضعها وأننا مرتاحون\nلكيفية تأثير ذلك على نتائجنا. دعونا نقيم\nخوارزمية الإشراف على المحتوى التي كتبناها. ما هي الافتراضات التي قمنا بها؟ حسنًا، نحن هنا نفضل الحسابات القديمة، الأشخاص الذين كانوا على\nالموقع لفترة من الوقت. تفترض الخوارزمية لدينا أن هؤلاء\nالمستخدمين أكثر جدارة بالثقة. الآن، نظرنا إلى البيانات التاريخية ووجدنا ارتباطًا هنا، ولا نعتقد أن\nعمر الحساب يرتبط بقوة بأي فئة محمية مثل\nالعرق أو الجنس أو الدين. لذلك قررت أنني مرتاح\nلهذا الافتراض. أنا على استعداد لقبول\nالظلم الطفيف تجاه المستخدمين الجدد ذوي النوايا الحسنة. ماذا عن التحقق من عدد الكلمات حيث يكون افتراض المشاركات التي تحتوي على\nالكثير من الكلمات أقل فائدة؟ الآن، قد يكون لهذا تأثير غير عادل بناءً على لغة المنشور، لأن بعض اللغات\nتميل إلى الحاجة إلى المزيد من الكلمات للتعبير عن نفس الفكرة. على سبيل المثال، الفرنسية\nغالبا ما تكون أكثر كلاما من الإنجليزية. وهذا تحيز ربما\nلا أرغب في قبوله. يضم موقعنا الكثير من المستخدمين\nمن جميع أنحاء العالم، ولا أريد تفضيل\nلغة على أخرى. لذلك قد أعود إلى\nلوحة الرسم مع هذا وإما أن أجد\nمعايير مختلفة لاستخدامها، أو أحاول تعديل\nحدود عدد الكلمات بشكل عادل بناءً على اللغة. عملية التقييم هذه مستمرة. أثناء تشغيل هذه الخوارزمية على موقعنا، نريد مراقبة الاتجاهات التي يتم فيها تمييز المنشورات\nوالتي تم وضع علامة عليها، وضبط الخوارزمية وفقًا لذلك استجابةً للبيانات الجديدة.",
    "t_italian": "- [Istruttore] Gli algoritmi\ncontrollano sempre più molte aree della nostra vita quotidiana, dalle richieste di prestito, alle app di appuntamenti, alle liste d'attesa ospedaliere. In quanto consumatori responsabili, e ora creatori di algoritmi, dobbiamo pensare in modo critico a come viene misurato il successo di\nun algoritmo. Quali ipotesi e pregiudizi\npotrebbero influenzare i risultati e in che modo ciò dovrebbe\ninfluire sul modo in cui li utilizziamo? Consideriamo un algoritmo di raccomandazione. Il tuo feed di notizie non ti chiede di valutare la tua soddisfazione\nper ciascun risultato, quindi non sa veramente come ti senti. Molti algoritmi valutano invece il successo sulla base di indicatori più quantificabili. Ad esempio, la frequenza con cui gli utenti\nfanno clic su un risultato specifico. Ma questo può portare a un falso ciclo di feedback in cui è\npiù probabile che gli algoritmi consiglino articoli con\ntitoli sensazionali o immagini carine di gatti perché attirano l'attenzione degli utenti, anche se quando aprono l'articolo sono estremamente\ninsoddisfatti del risultato. In definitiva, questo algoritmo\nmisura il successo in base al coinvolgimento e\nnon all’effettiva utilità. Ciò non è necessariamente sbagliato,\nma influenzerà sicuramente i tipi di risultati visualizzati dagli utenti. Quindi, mentre consumi la tecnologia, mantieni un sano scetticismo\ne chiediti: \"Perché l'algoritmo\nmi offre questo risultato?\" Ok, quindi gli esseri umani sono di parte, ma cosa succederebbe se chiedessi semplicemente all'intelligenza artificiale di\nprogettare il mio algoritmo per me? Ebbene, le IA vengono addestrate sui dati\ncreati dagli esseri umani, spesso contenuti su Internet. Ricorda che i computer vedono tutti i dati, siano essi testuali, audio o visivi, come una sequenza di numeri.  Non capiscono\ncosa sta succedendo in una foto nello stesso modo in cui lo capisce un essere umano. Nella maggior parte dei casi, gli algoritmi generati dall'intelligenza artificiale cercano solo modelli nei dati, indipendentemente dal fatto che esista\no meno una relazione causale. Ad esempio, un\nalgoritmo di assunzione sperimentale ha scoperto che l’intelligenza artificiale favoriva i candidati uomini, declassando i curriculum che\nincludevano termini come “donne” o che facevano riferimento a tutte le università femminili. Il gruppo di\ningegneri esistente presso l'azienda era prevalentemente maschile, quindi quando l'intelligenza artificiale si è formata\nsui dati di assunzioni precedenti, ha trovato un modello che\nriteneva significativo: non assumere donne. In pratica, gli algoritmi di intelligenza artificiale possono effettivamente amplificare i pregiudizi storici\nperché i dati di formazione disponibili tendono a restare indietro rispetto al\nmomento culturale attuale e distorcono pesantemente l’inglese, il che significa che altre culture e lingue sono meno rappresentate. Questo non vuol dire che gli\nalgoritmi generati dall’uomo siano sempre migliori di quelli generati dall’intelligenza artificiale, ma gli algoritmi dell’intelligenza artificiale tendono\nad essere meno trasparenti, quindi è più importante\nritenere le organizzazioni responsabili del monitoraggio dei loro pregiudizi. Come programmatori, come possiamo limitare i pregiudizi negli algoritmi che progettiamo? È impossibile modellare perfettamente il mondo reale in un programma. Bisognerà sempre fare qualche presupposto e qualche semplificazione. Vogliamo solo assicurarci di riconoscere le ipotesi che stiamo facendo e di essere a nostro agio con il\nmodo in cui ciò influisce sui nostri risultati. Valutiamo questo\nalgoritmo di moderazione dei contenuti che abbiamo scritto. Quali ipotesi abbiamo fatto? Bene, qui stiamo favorendo gli account più vecchi, persone che sono sul\nsito da un po'. Il nostro algoritmo presuppone che tali\nutenti siano più affidabili. Ora, abbiamo esaminato i dati storici e abbiamo trovato una correlazione qui, e non pensiamo che l'\netà dell'account sia fortemente correlata a qualsiasi classe protetta come\nrazza, genere o religione. Quindi ho deciso che mi sento a mio agio\ncon questo presupposto. Sono disposto ad accettare\nla leggera ingiustizia nei confronti dei nuovi utenti ben intenzionati. Che ne dici di questo controllo del conteggio delle parole in cui presupporre che i post con\nmolte parole siano meno utili? Ora, questo potrebbe avere un impatto ingiusto in base alla lingua del post, perché alcune lingue\ntendono ad aver bisogno di più parole per esprimere la stessa idea. Ad esempio, il francese è\nspesso più prolisso dell’inglese. È un pregiudizio che forse\nnon sono disposto ad accettare. Il nostro sito ha molti utenti\nda tutto il mondo e non voglio favorire\nuna lingua rispetto a un'altra. Quindi potrei tornare al\ntavolo da disegno con questo e trovare\ncriteri diversi da utilizzare o provare a regolare in modo equo\ni limiti del conteggio delle parole in base alla lingua. Questo processo di valutazione è in corso. Mentre questo algoritmo viene eseguito sul nostro sito, vogliamo monitorare le tendenze in cui i post sono presenti\ne quali sono contrassegnati, e adattare l'algoritmo di conseguenza in risposta ai nuovi dati."
}